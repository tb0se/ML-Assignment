{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width: 100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width: 100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "We use the Naive Bayes Classifier to determine whether a job posting is fraudulent or real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_excel('Job_Postings(clean).xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The features and the target class of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: \n",
      " ['job_id' 'title' 'location' 'department' 'salary_range' 'company_profile'\n",
      " 'description' 'requirements' 'benefits' 'telecommuting'\n",
      " 'has_company_logo' 'has_questions' 'employment_type'\n",
      " 'required_experience' 'required_education' 'industry' 'function'] \n",
      "\n",
      "target class: \n",
      " fraudulent\n"
     ]
    }
   ],
   "source": [
    "features = dataset.columns[1:dataset.columns.shape[0]-1].to_numpy()\n",
    "print('features:', '\\n', features, '\\n')\n",
    "\n",
    "target_class = dataset.columns[dataset.columns.shape[0]-1]\n",
    "print('target class:', '\\n', target_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:  (10729, 19)\n",
      "Validation data:  (5722, 19)\n",
      "Test data:  (1429, 19) \n",
      "\n",
      "training data example item: \n",
      "\n",
      " Unnamed: 0                                                             0\n",
      "job_id                                                                 1\n",
      "title                                                   marketing intern\n",
      "location                                                   us,ny,newyork\n",
      "department                                                     marketing\n",
      "salary_range                                                     missing\n",
      "company_profile        were food52 and weve created a groundbreaking ...\n",
      "description            food52 a fastgrowing james beard awardwinning ...\n",
      "requirements           experience with content management systems a m...\n",
      "benefits                                                         missing\n",
      "telecommuting                                                          0\n",
      "has_company_logo                                                       1\n",
      "has_questions                                                          0\n",
      "employment_type                                                    other\n",
      "required_experience                                           internship\n",
      "required_education                                               missing\n",
      "industry                                                         missing\n",
      "function                                                       marketing\n",
      "fraudulent                                                             0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "training_data = dataset.loc[:10728]\n",
    "validation_data = dataset.loc[10729:16450]\n",
    "test_data = dataset.loc[16451:]\n",
    "\n",
    "print(\"Training data: \",training_data.shape)\n",
    "print(\"Validation data: \",validation_data.shape)\n",
    "print(\"Test data: \",test_data.shape,'\\n')\n",
    "\n",
    "print('training data example item:','\\n\\n', training_data.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Probabilities:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Compute Prior Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels:  [0 1]\n",
      "Number of real job posts:  10268\n",
      "Number of fraudulent job posts:  461 \n",
      "\n",
      "Prior probability job post is REAL:  0.9570323422499767\n",
      "Prior probability job post is FRAUDULENT:  0.042967657750023314\n"
     ]
    }
   ],
   "source": [
    "class_labels, class_labels_counts = np.unique(training_data['fraudulent'], return_counts=True)\n",
    "\n",
    "prior_prob_real = class_labels_counts[0] / np.sum(class_labels_counts)\n",
    "prior_prob_fraud = 1 - prior_prob_real\n",
    "\n",
    "print('Class labels: ', class_labels)\n",
    "print(\"Number of real job posts: \",class_labels_counts[0])\n",
    "print(\"Number of fraudulent job posts: \",class_labels_counts[1],\"\\n\")\n",
    "print('Prior probability job post is REAL: ', prior_prob_real)\n",
    "print('Prior probability job post is FRAUDULENT: ', prior_prob_fraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Class Conditional Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HELPER FUNCTIONS\n",
    "- Compute fraudulent and real counts of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_counts(class_labels, feature_full_data, feature_unique_data):\n",
    "    \n",
    "    # Get index of the classes\n",
    "    real_index = np.where(class_labels == 0)[0]\n",
    "    fraud_index = np.where(class_labels == 1)[0]\n",
    "    \n",
    "    # Get the real and fraudulent features\n",
    "    feature_real = feature_full_data[real_index]\n",
    "    feature_fraud = feature_full_data[fraud_index]\n",
    "    \n",
    "    # Get unique real and fraudulent features\n",
    "    unique_feature_real, real_counts =  np.unique(feature_real ,return_counts=True)\n",
    "    unique_feature_fraud, fraud_counts =  np.unique(feature_fraud ,return_counts=True)\n",
    "    \n",
    "    # Get the indices\n",
    "    real_bool = np.isin(feature_unique_data,unique_feature_real)\n",
    "    real_index = np.where(real_bool == True)[0]\n",
    "    fraud_bool = np.isin(feature_unique_data,unique_feature_fraud)\n",
    "    fraud_index = np.where(fraud_bool == True)[0]\n",
    "        \n",
    "    # Initialise counts\n",
    "    unique_real_counts = np.zeros(feature_unique_data.shape)\n",
    "    unique_fraud_counts = np.zeros(feature_unique_data.shape)\n",
    "    \n",
    "    # create unique counts\n",
    "    unique_real_counts[real_index] = real_counts\n",
    "    unique_fraud_counts[fraud_index] = fraud_counts\n",
    "    \n",
    "    # create the pandas object\n",
    "    result_object = pd.DataFrame({'Real':unique_real_counts,\n",
    "                                 'Fraudulent':unique_fraud_counts},\n",
    "                                 index =feature_unique_data )\n",
    "    return result_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute fraudulent and real counts of each the descriptive features(with long text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_text(class_labels,feature_data):\n",
    "    \n",
    "    # Get index of the classes\n",
    "    real_class_index = np.where(class_labels == 0)[0]\n",
    "    fraud_class_index = np.where(class_labels == 1)[0]\n",
    "    \n",
    "    # Get the real and fraudulent features\n",
    "    feature_real = feature_data[real_class_index].str.split()\n",
    "    feature_fraud = feature_data[fraud_class_index].str.split()\n",
    "    \n",
    "    # Create text that convey a particular tone (fraud/real)\n",
    "    real_text = feature_real.explode().to_numpy()\n",
    "    fraud_text = feature_fraud.explode().to_numpy()\n",
    "    \n",
    "    # Find unique words and word counts\n",
    "    unique_real_text,real_counts = np.unique(real_text,return_counts = True)\n",
    "    unique_fraud_text,fraud_counts = np.unique(fraud_text,return_counts = True)\n",
    "    print(\"Real Counts:\",real_counts.shape)\n",
    "    print(\"Fraud Counts:\",fraud_counts.shape)\n",
    "    \n",
    "    # Find all unique words\n",
    "    temp_words = np.append(unique_real_text,unique_fraud_text)\n",
    "    all_unique_words = np.unique(temp_words)\n",
    "    tot_all_unique_words = len(all_unique_words)\n",
    "    \n",
    "    \n",
    "    # Get the indices of real or fraudelent words in all unique words\n",
    "    real_bool = np.isin(all_unique_words,unique_real_text)\n",
    "    real_index = np.where(real_bool == True)[0]\n",
    "    fraud_bool = np.isin(all_unique_words,unique_fraud_text)\n",
    "    fraud_index = np.where(fraud_bool == True)[0]\n",
    "#     print(fraud_index)\n",
    "        \n",
    "    # Initialise the unique counts\n",
    "    unique_real_counts = np.zeros(all_unique_words.shape)\n",
    "    unique_fraud_counts = np.zeros(all_unique_words.shape)\n",
    "#     print(\"Unique real counts:\",unique_real_counts.shape)\n",
    "#     print(\"Unique fraud counts:\",unique_fraud_counts.shape)\n",
    "    \n",
    "    # Assign the counts to their repective indices\n",
    "    unique_real_counts[real_index] = real_counts\n",
    "    unique_fraud_counts[fraud_index] = fraud_counts\n",
    "    \n",
    "    # Make pandas object to store this data object\n",
    "    result_object = pd.DataFrame({'Real':unique_real_counts,\n",
    "                                 'Fraudulent':unique_fraud_counts},\n",
    "                                 index =all_unique_words )\n",
    "    return result_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Compute probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probabilities(feature_values_fraud_counts):\n",
    "    \n",
    "    #Temporary values\n",
    "    fraud_arr = feature_values_fraud_counts['Fraudulent']\n",
    "    real_arr = feature_values_fraud_counts['Real']\n",
    "\n",
    "    \n",
    "    # Init the probabilities\n",
    "    prob_real = np.zeros(feature_values_fraud_counts.shape[0])\n",
    "    prob_fraud = np.zeros(feature_values_fraud_counts.shape[0])\n",
    "    \n",
    "    # (1)Find index of zero values for  laplacian smoothing\n",
    "    fraud_zero_index = np.where( fraud_arr == 0)[0]\n",
    "    real_zero_index = np.where(real_arr == 0)[0]\n",
    "    \n",
    "    # apply laplacian by adding a fake datapoint count to both counts of each class label\n",
    "    feature_value_freq_laplacian_real = np.add(fraud_arr[real_zero_index],real_arr[real_zero_index])+2\n",
    "    feature_value_freq_laplacian_fraud = np.add(fraud_arr[fraud_zero_index],real_arr[fraud_zero_index])+2\n",
    "    #Calculate probabilites\n",
    "    prob_real[real_zero_index] = (real_arr[real_zero_index]+1)/feature_value_freq_laplacian_real\n",
    "    prob_fraud[fraud_zero_index] = (fraud_arr[fraud_zero_index]+1)/feature_value_freq_laplacian_fraud\n",
    "    \n",
    "    # (2)Find index of non-zero values\n",
    "    fraud_nonzero_index = np.where( fraud_arr != 0)[0]\n",
    "    real_nonzero_index = np.where(real_arr != 0)[0]\n",
    "    \n",
    "    #Calculate probabilites\n",
    "    feature_value_freq_real = np.add(fraud_arr[real_nonzero_index],real_arr[real_nonzero_index])\n",
    "    feature_value_freq_fraud = np.add(fraud_arr[fraud_nonzero_index],real_arr[fraud_nonzero_index])\n",
    "    \n",
    "    prob_real[real_nonzero_index] = real_arr[real_nonzero_index]/feature_value_freq_real\n",
    "    prob_fraud[fraud_nonzero_index] = fraud_arr[fraud_nonzero_index]/feature_value_freq_fraud\n",
    "\n",
    "    \n",
    "    # Create pandas object\n",
    "    result_object = pd.DataFrame({'RealProb':prob_real,\n",
    "                                 'FraudulentProb':prob_fraud},\n",
    "                                 index =feature_values_fraud_counts.index)\n",
    "    return result_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Title Feature\n",
    "Compute fraud and real counts of each unique title with respect to the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of unique titles: (6921,)\n",
      "                                                    Real  Fraudulent\n",
      "$950/week truck drivers needed                       1.0         0.0\n",
      "$950/week. truck drivers needed                      1.0         0.0\n",
      "(assistant) accountant                               1.0         0.0\n",
      "(internship) communication / e-marketing assistant   1.0         0.0\n",
      "(internship) pr / event manager assistant            1.0         0.0\n"
     ]
    }
   ],
   "source": [
    "training_data_titles = training_data['title']\n",
    "unique_titles = np.unique(training_data_titles)\n",
    "print(\"No of unique titles:\",unique_titles.shape)\n",
    "title_counts = compute_counts(training_data['fraudulent'], training_data_titles, unique_titles)\n",
    "print(title_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Location feature\n",
    "Compute fraud and real counts of each unique location with respect to the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Real  Fraudulent\n",
      "ae                        3.0         0.0\n",
      "ae,az,abudhabi            1.0         0.0\n",
      "ae,du                     4.0         0.0\n",
      "ae,du,dubai              16.0         0.0\n",
      "ae,du,dubaiinternetcity   2.0         0.0\n"
     ]
    }
   ],
   "source": [
    "training_data_locations = training_data['location']\n",
    "unique_locations = np.unique(training_data_locations)\n",
    "location_counts = compute_counts(training_data['fraudulent'], training_data_locations, unique_locations)\n",
    "print(location_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Department feature\n",
    "Compute fraud and real counts of each unique department with respect to the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Real  Fraudulent\n",
      "\\tcorporate shared services   0.0         1.0\n",
      "(consultant)                  1.0         0.0\n",
      ".net development              1.0         0.0\n",
      "0                             1.0         0.0\n",
      "1221                          0.0         1.0\n"
     ]
    }
   ],
   "source": [
    "training_data_departments = training_data['department']\n",
    "unique_departments = np.unique(training_data_departments)\n",
    "department_counts = compute_counts(training_data['fraudulent'], training_data_departments, unique_departments)\n",
    "print(department_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Salary Range feature\n",
    "Compute fraud and real counts of each unique value in salary_range feature with respect to the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Real  Fraudulent\n",
      "0-0       76.0         0.0\n",
      "0-1        1.0         0.0\n",
      "0-1000     2.0         0.0\n",
      "0-100000   3.0         0.0\n",
      "0-110406   1.0         0.0\n"
     ]
    }
   ],
   "source": [
    "training_data_salary_ranges = training_data['salary_range']\n",
    "unique_salary_ranges = np.unique(training_data_salary_ranges)\n",
    "salary_range_counts = compute_counts(training_data['fraudulent'], training_data_salary_ranges, unique_salary_ranges)\n",
    "print(salary_range_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) Telecommuting feature\n",
    "Compute fraud and real counts of each unique value in telecommuting feature with respect to the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Real  Fraudulent\n",
      "0  9831.0       421.0\n",
      "1   437.0        40.0\n"
     ]
    }
   ],
   "source": [
    "training_data_telecommuting = training_data['telecommuting']\n",
    "unique_telecommuting = np.unique(training_data['telecommuting'])\n",
    "telecommuting_counts = compute_counts(training_data['fraudulent'], training_data_telecommuting, unique_telecommuting)\n",
    "print(telecommuting_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6) Company logo feature\n",
    "Compute fraud and real counts of each unique value in has_company_logo feature with respect to the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Real  Fraudulent\n",
      "0  1873.0       275.0\n",
      "1  8395.0       186.0\n"
     ]
    }
   ],
   "source": [
    "training_data_company_logo = training_data['has_company_logo']\n",
    "unique_company_logo = np.unique(training_data_company_logo)\n",
    "company_logo_counts = compute_counts(training_data['fraudulent'], training_data_company_logo, unique_company_logo)\n",
    "print(company_logo_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (7) Questions feature\n",
    "Compute fraud and real counts of each unique value in has_questions feature with respect to the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Real  Fraudulent\n",
      "0  4702.0       310.0\n",
      "1  5566.0       151.0\n"
     ]
    }
   ],
   "source": [
    "training_data_questions = training_data['has_questions']\n",
    "unique_questions = np.unique(training_data_questions)\n",
    "questions_counts = compute_counts(training_data['fraudulent'], training_data_questions, unique_questions)\n",
    "print(questions_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (8) Employment Type feature\n",
    "Compute fraud and real counts of each unique employment_type feature value with respect to the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Real  Fraudulent\n",
      "contract   1040.0        26.0\n",
      "full-time  6405.0       289.0\n",
      "missing    2098.0       124.0\n",
      "other       128.0         4.0\n",
      "part-time   445.0        17.0\n"
     ]
    }
   ],
   "source": [
    "training_data_employment_type = training_data['employment_type']\n",
    "unique_employment_type = np.unique(training_data_employment_type)\n",
    "employment_type_counts = compute_counts(training_data['fraudulent'], training_data_employment_type, unique_employment_type)\n",
    "print(employment_type_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (9) Required Experience feature\n",
    "Compute fraud and real counts of each unique required_experience feature value with respect to the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Real  Fraudulent\n",
      "associate    1379.0        24.0\n",
      "director      232.0        11.0\n",
      "entry level  1610.0        91.0\n",
      "executive      80.0         8.0\n",
      "internship    230.0         6.0\n"
     ]
    }
   ],
   "source": [
    "training_data_required_experience = training_data['required_experience']\n",
    "unique_required_experience = np.unique(training_data_required_experience)\n",
    "required_experience_counts = compute_counts(training_data['fraudulent'], training_data_required_experience, unique_required_experience)\n",
    "print(required_experience_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (10) Required Education feature\n",
    "Compute fraud and real counts of each unique required_education feature value with respect to the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Real  Fraudulent\n",
      "associate degree            148.0         5.0\n",
      "bachelor's degree          3192.0        62.0\n",
      "certification                80.0        11.0\n",
      "doctorate                    20.0         0.0\n",
      "high school or equivalent  1158.0        98.0\n"
     ]
    }
   ],
   "source": [
    "training_data_required_education = training_data['required_education']\n",
    "unique_required_education = np.unique(training_data_required_education)\n",
    "required_education_counts = compute_counts(training_data['fraudulent'], training_data_required_education, unique_required_education)\n",
    "print(required_education_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (11) Industry feature\n",
    "Compute fraud and real counts of each unique industry feature value with respect to the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Real  Fraudulent\n",
      "accounting               62.0        25.0\n",
      "airlines/aviation        26.0         0.0\n",
      "animation                 2.0         0.0\n",
      "apparel & fashion        49.0         0.0\n",
      "architecture & planning   6.0         0.0\n"
     ]
    }
   ],
   "source": [
    "training_data_industry = training_data['industry']\n",
    "unique_industry = np.unique(training_data_industry)\n",
    "industry_counts = compute_counts(training_data['fraudulent'], training_data_industry, unique_industry)\n",
    "print(industry_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (12) Function feature\n",
    "Compute fraud and real counts of each unique function feature value with respect to the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Real  Fraudulent\n",
      "accounting/auditing  130.0         8.0\n",
      "administrative       347.0        51.0\n",
      "advertising           59.0         3.0\n",
      "art/creative          72.0         0.0\n",
      "business analyst      45.0         1.0\n"
     ]
    }
   ],
   "source": [
    "training_data_function = training_data['function']\n",
    "unique_function = np.unique(training_data_function)\n",
    "function_counts = compute_counts(training_data['fraudulent'], training_data_function, unique_function)\n",
    "print(function_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (14) Company profile feature\n",
    "Compute fraud and real counts of each unique word in company profile feature value with respect to the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Counts: (17787,)\n",
      "Fraud Counts: (1242,)\n",
      "                                                    Real  Fraudulent\n",
      "0                                                   14.0         0.0\n",
      "000                                                 28.0         0.0\n",
      "00phone_21b21d2fe8e914fb2fc8e5e48952a8b26469b7d...   1.0         0.0\n",
      "00phone_c61d5134628fb74682f7394c6bd4e383753eb89...   1.0         0.0\n",
      "02                                                   2.0         0.0\n"
     ]
    }
   ],
   "source": [
    "training_data_company_profile = training_data['company_profile']\n",
    "company_profile_counts = count_text(training_data['fraudulent'],training_data_company_profile)\n",
    "print(company_profile_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (14) Description feature\n",
    "Compute fraud and real counts of each unique word in description feature value with respect to the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Counts: (78015,)\n",
      "Fraud Counts: (8432,)\n",
      "               Real  Fraudulent\n",
      "0              54.0         1.0\n",
      "000             8.0         2.0\n",
      "004contact      1.0         0.0\n",
      "00592duration   2.0         0.0\n",
      "01761           1.0         0.0\n"
     ]
    }
   ],
   "source": [
    "training_data_description = training_data['description']\n",
    "description_counts = count_text(training_data['fraudulent'],training_data_description)\n",
    "print(description_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (15) Requirements feature\n",
    "Compute fraud and real counts of each unique word in requirements feature value with respect to the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Counts: (54525,)\n",
      "Fraud Counts: (5313,)\n",
      "         Real  Fraudulent\n",
      "0        51.0         0.0\n",
      "0062834   1.0         0.0\n",
      "01        1.0         0.0\n",
      "01252     1.0         0.0\n",
      "02        2.0         0.0\n"
     ]
    }
   ],
   "source": [
    "training_data_requirements = training_data['requirements'].astype(np.unicode)\n",
    "requirements_counts = count_text(training_data['fraudulent'],training_data_requirements)\n",
    "print(requirements_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (16) Benefits feature\n",
    "Compute fraud and real counts of each unique word in benefits feature value with respect to the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Counts: (17613,)\n",
      "Fraud Counts: (1714,)\n",
      "                Real  Fraudulent\n",
      "0               19.0         0.0\n",
      "000              4.0         1.0\n",
      "0062953          1.0         0.0\n",
      "01772724252job   1.0         0.0\n",
      "025              4.0         0.0\n"
     ]
    }
   ],
   "source": [
    "training_data_benefits = training_data['benefits'].astype(np.unicode)\n",
    "benefits_counts = count_text(training_data['fraudulent'],training_data_benefits)\n",
    "print(benefits_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compute probabilities\n",
    "Compute probabilities of each unique feature value with respect to the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    RealProb  FraudulentProb\n",
      "$950/week truck drivers needed                           1.0        0.333333\n",
      "$950/week. truck drivers needed                          1.0        0.333333\n",
      "(assistant) accountant                                   1.0        0.333333\n",
      "(internship) communication / e-marketing assistant       1.0        0.333333\n",
      "(internship) pr / event manager assistant                1.0        0.333333\n"
     ]
    }
   ],
   "source": [
    "titles_prob = compute_probabilities(title_counts)\n",
    "print(titles_prob.head())\n",
    "locations_prob = compute_probabilities(location_counts)\n",
    "departments_prob = compute_probabilities(department_counts)\n",
    "salary_range_prob = compute_probabilities(salary_range_counts)\n",
    "telecommuting_prob = compute_probabilities(telecommuting_counts)\n",
    "company_logo_prob = compute_probabilities(company_logo_counts)\n",
    "questions_prob = compute_probabilities(questions_counts)\n",
    "employment_type_prob = compute_probabilities(employment_type_counts)\n",
    "required_experience_prob = compute_probabilities(required_experience_counts)\n",
    "required_education_prob = compute_probabilities(required_education_counts)\n",
    "industry_prob = compute_probabilities(industry_counts)\n",
    "functions_prob = compute_probabilities(function_counts)\n",
    "\n",
    "# Sentiment probabilities\n",
    "company_profile_prob = compute_probabilities(company_profile_counts)\n",
    "description_prob = compute_probabilities(description_counts)\n",
    "requirements_prob = compute_probabilities(requirements_counts)\n",
    "benefits_prob = compute_probabilities(benefits_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make an inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'location', 'department', 'salary_range', 'telecommuting', 'has_company_logo', 'has_questions', 'employment_type', 'required_experience', 'required_education', 'industry', 'function'] \n",
      "\n",
      "['company_profile', 'description', 'requirements', 'benefits']\n"
     ]
    }
   ],
   "source": [
    "all_probabilities = []\n",
    "all_probabilities.append(titles_prob)\n",
    "all_probabilities.append(locations_prob)\n",
    "all_probabilities.append(departments_prob)\n",
    "all_probabilities.append(salary_range_prob)\n",
    "all_probabilities.append(telecommuting_prob)\n",
    "all_probabilities.append(company_logo_prob)\n",
    "all_probabilities.append(questions_prob)\n",
    "all_probabilities.append(employment_type_prob)\n",
    "all_probabilities.append(required_experience_prob)\n",
    "all_probabilities.append(required_education_prob)\n",
    "all_probabilities.append(industry_prob)\n",
    "all_probabilities.append(functions_prob)\n",
    "\n",
    "senti_probabilities = []\n",
    "senti_probabilities.append(company_profile_prob)\n",
    "senti_probabilities.append(description_prob)\n",
    "senti_probabilities.append(requirements_prob)\n",
    "senti_probabilities.append(benefits_prob)\n",
    "\n",
    "normal_features = []\n",
    "for i in range(features.shape[0]):\n",
    "    if i not in range(5,9) and i != features.shape[0] and i != 0:\n",
    "        normal_features.append(features[i])\n",
    "print(normal_features,\"\\n\")\n",
    "\n",
    "sentiment_features = ['company_profile','description','requirements','benefits']\n",
    "print(sentiment_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for sentiment analysis\n",
    "\n",
    "def calc_likelihood(encoded_arr,prob_arr):\n",
    "#     print(\"Encoded array:\",encoded_arr.shape)\n",
    "    \n",
    "    # Words that exist index\n",
    "    word_index = np.where(encoded_arr == '1')[0]\n",
    "#     print(\"Word index\",word_index.shape)\n",
    "    \n",
    "    # Fraud likelihood\n",
    "    fraud  = prob_arr.iloc[word_index]['FraudulentProb']\n",
    "    fraud_arr = fraud.to_numpy()\n",
    "    fraud_likelihood = np.prod(fraud_arr)\n",
    "#     print(\"Fraud likelihood:\",fraud_likelihood)\n",
    "    \n",
    "    # True likelihood\n",
    "    true  = prob_arr.iloc[word_index]['RealProb']\n",
    "    true_arr = true.to_numpy()\n",
    "    true_likelihood = np.prod(true_arr)\n",
    "#     print(\"True likelihood:\",true_likelihood,\"\\n\")\n",
    "    \n",
    "    return fraud_likelihood,true_likelihood\n",
    "    \n",
    "\n",
    "\n",
    "def sentiment_analysis(prob_arr,feature):\n",
    "    \n",
    "    # Split into words\n",
    "    feature_arr = feature.split()\n",
    "    \n",
    "    # Unique words \n",
    "    feature_unique_words = np.unique(feature_arr)\n",
    "#     print(\"feature unique words:\",feature_unique_words.shape)\n",
    "    \n",
    "    # Init encoded array\n",
    "    encoded_array = np.full(len(prob_arr),'0')\n",
    "#     print(\"Encoded array\",encoded_array.shape)\n",
    "    \n",
    "    # Find words index\n",
    "    bool_words = np.isin(prob_arr.index,feature_unique_words)\n",
    "    index_words = np.where(bool_words == True)[0]\n",
    "#     print(\"Index of words:\",index_words.shape)\n",
    "    \n",
    "    # Encode (if word exists in train data)\n",
    "    encoded_array[index_words] = '1'\n",
    "    \n",
    "    # Calculate likelihood\n",
    "    prob_fraud , prob_real = calc_likelihood(encoded_array,prob_arr)\n",
    "    \n",
    "    return prob_fraud,prob_real\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_calc(prob_arr,feature):\n",
    "    if(feature in prob_arr.index):\n",
    "        prob_fraud = prob_arr.loc[feature]['FraudulentProb']\n",
    "        prob_real = prob_arr.loc[feature]['RealProb']\n",
    "    else:\n",
    "        # Value doesnt exist in training data\n",
    "        # Smoothing\n",
    "        tot_fraud_job_posts = class_labels_counts[1]\n",
    "        tot_real_job_posts = class_labels_counts[0]\n",
    "        no_unique_feature_values = prob_arr.shape[0]\n",
    "        \n",
    "        prob_fraud = 1/(tot_fraud_job_posts + no_unique_feature_values)\n",
    "        prob_real = 1/(tot_real_job_posts + no_unique_feature_values)\n",
    "    return prob_fraud ,prob_real\n",
    "\n",
    "def retreive_probabilities(data_point):\n",
    "    \n",
    "    # Probabilities\n",
    "    prob_fraud = np.array([])\n",
    "    prob_real = np.array([])\n",
    "    \n",
    "    #print(data_point)\n",
    "    \n",
    "    # Seperate features\n",
    "    normal_features_data = data_point.loc[normal_features]\n",
    "    sentiment_features_data = data_point.loc[sentiment_features].astype(np.unicode)\n",
    "    # Ensure feature is string\n",
    "#     feature = feature.astype(np.unicode)\n",
    "\n",
    "    # Normal probabilities\n",
    "    for i in range(len(all_probabilities)):\n",
    "        norm_fraud, norm_real = feature_calc(all_probabilities[i],normal_features_data[i])\n",
    "        prob_fraud = np.append(prob_fraud,norm_fraud)\n",
    "        prob_real = np.append(prob_real,norm_real)\n",
    "        \n",
    "    # Sentiment probabilites\n",
    "    for i in range(len(senti_probabilities)):\n",
    "        senti_fraud, senti_real = sentiment_analysis(senti_probabilities[i],sentiment_features_data[i])\n",
    "        prob_fraud = np.append(prob_fraud,senti_fraud)\n",
    "        prob_real = np.append(prob_real,senti_real)\n",
    "        \n",
    "    \n",
    "    return prob_fraud,prob_real\n",
    "\n",
    "\n",
    "# This function makes the final decision\n",
    "def final_decision(prob_fraud_arr,prob_real_arr):\n",
    "    \n",
    "    # Final decision\n",
    "    final_decision = -1\n",
    "    \n",
    "    # Likelihoods\n",
    "    likelihood_fraud = np.prod(prob_fraud_arr)\n",
    "    likelihood_real = np.prod(prob_real_arr)\n",
    "    \n",
    "    # Posterior probability\n",
    "    post_prob_fraud = ( likelihood_fraud*prior_prob_fraud )/( likelihood_fraud*prior_prob_fraud + likelihood_real*prior_prob_real)\n",
    "    post_prob_real = 1-post_prob_fraud\n",
    "#     post_prob_real = ( likelihood_real*prior_prob_real )/( likelihood_fraud*prior_prob_fraud + likelihood_real*prior_prob_real) \n",
    "\n",
    "    # Final decision\n",
    "    if post_prob_fraud>post_prob_real:\n",
    "        final_decision = 1\n",
    "    else:\n",
    "        final_decision = 0\n",
    "    \n",
    "    return final_decision\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(dataset):\n",
    "    \n",
    "    # Final predications\n",
    "    final_predications = np.array([])\n",
    "    \n",
    "    for i in range(dataset.shape[0]):\n",
    "        data_point = dataset.iloc[i]\n",
    "        probability_fraud, probability_real = retreive_probabilities(data_point)\n",
    "        final_predications = np.append( final_predications,final_decision(probability_fraud,probability_real) )\n",
    "        \n",
    "        \n",
    "    return final_predications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "test_data_pred = classify(test_data)\n",
    "print(test_data_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1429,)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target_values = test_data['fraudulent'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1094    0]\n",
      " [ 335    0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(test_target_values,test_data_pred,class_labels)\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title(\"Confusion Matrix (Test Data)\")\n",
    "fig.colorbar(cax)\n",
    "# ax.set_xticklabels(['']+class_labels)\n",
    "# ax.set_yticklabels(['']+class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
